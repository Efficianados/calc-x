{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "import scipy.sparse\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "\n",
    "ds_names = [\n",
    "    \"MU-NLPC/Calc-gsm8k\",\n",
    "    \"MU-NLPC/Calc-aqua_rat\",\n",
    "    \"MU-NLPC/Calc-math_qa\",\n",
    "    \"MU-NLPC/Calc-ape210k\",\n",
    "    \"MU-NLPC/Calc-mawps\",\n",
    "    \"MU-NLPC/Calc-svamp\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_symbols = set(string.ascii_lowercase.lower() + \" \")\n",
    "dss = {}\n",
    "split_names = set()\n",
    "\n",
    "for full_name in ds_names:\n",
    "    ds = datasets.load_dataset(full_name)\n",
    "    ds_name = full_name.split(\"/\")[-1].lower()\n",
    "    for split_name, split in ds.items():\n",
    "        split_names.add(split_name)\n",
    "        key = ds_name, split_name\n",
    "        dss[key] = split.to_pandas()[[\"question\", \"chain\", \"result\"]]\n",
    "        dss[key][\"question_simplified\"] = (\n",
    "            dss[key][\"question\"]\n",
    "            .str.encode(\"ascii\", errors=\"ignore\")\n",
    "            .str.decode(\"ascii\")\n",
    "            .str.lower()\n",
    "            .str.split()\n",
    "            .str.join(\" \")\n",
    "            .apply(lambda text: \"\".join([c for c in text if c in keep_symbols]))\n",
    "            .str.split()\n",
    "            .str.join(\" \")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_ngrams_vectorizer = sklearn.feature_extraction.text.CountVectorizer(binary=True, dtype=np.int32, ngram_range=(1, 2))\n",
    "\n",
    "bow_ngrams_vectorizer.fit(\n",
    "    itertools.chain.from_iterable(ds[\"question_simplified\"] for ds in dss.values())\n",
    ")\n",
    "\n",
    "bows = {}\n",
    "\n",
    "for key, ds in dss.items():\n",
    "    bows[key] = bow_ngrams_vectorizer.transform(ds[\"question_simplified\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_jaccard_sim(bows_1: scipy.sparse.csr_matrix, bows_2: scipy.sparse.csr_matrix) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Jaccard distance between each row of X matrix and each row of Y matrix.\n",
    "    \"\"\"\n",
    "    sizes_of_1 = bows_1.getnnz(axis=1).astype(np.float32)\n",
    "    sizes_of_2 = bows_2.getnnz(axis=1).astype(np.float32)\n",
    "    intersect = (bows_1 @ bows_2.T).toarray().astype(np.float32)\n",
    "    union = sizes_of_1.reshape(-1, 1) + sizes_of_2.reshape(1, -1) - intersect\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        result = intersect / union\n",
    "        np.nan_to_num(result, nan=0, posinf=0, neginf=0, copy=False)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_highest_k_matches(scores: torch.Tensor, k: int):\n",
    "    top_in_rows = torch.topk(k=k, dim=1, sorted=False, largest=True, input=scores)\n",
    "    top_in_cols = torch.topk(k=k, dim=1, sorted=False, largest=True, input=scores.T)\n",
    "    return top_in_rows, top_in_cols\n",
    "\n",
    "\n",
    "def check_leak(bows_1, bows_2, top_k=10):\n",
    "    scores = pairwise_jaccard_sim(bows_1, bows_2)\n",
    "    return get_highest_k_matches(torch.tensor(scores), k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('calc-gsm8k', 'train'), ('calc-gsm8k', 'test')),\n",
      " (('calc-gsm8k', 'train'), ('calc-aqua_rat', 'test')),\n",
      " (('calc-gsm8k', 'train'), ('calc-aqua_rat', 'validation')),\n",
      " (('calc-gsm8k', 'train'), ('calc-math_qa', 'test')),\n",
      " (('calc-gsm8k', 'train'), ('calc-math_qa', 'validation')),\n",
      " (('calc-gsm8k', 'train'), ('calc-ape210k', 'test')),\n",
      " (('calc-gsm8k', 'train'), ('calc-ape210k', 'validation')),\n",
      " (('calc-gsm8k', 'train'), ('calc-mawps', 'validation')),\n",
      " (('calc-gsm8k', 'train'), ('calc-mawps', 'test')),\n",
      " (('calc-gsm8k', 'train'), ('calc-svamp', 'test')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-gsm8k', 'test')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-aqua_rat', 'test')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-aqua_rat', 'validation')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-math_qa', 'test')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-math_qa', 'validation')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-ape210k', 'test')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-ape210k', 'validation')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-mawps', 'validation')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-mawps', 'test')),\n",
      " (('calc-aqua_rat', 'train'), ('calc-svamp', 'test')),\n",
      " (('calc-math_qa', 'train'), ('calc-gsm8k', 'test')),\n",
      " (('calc-math_qa', 'train'), ('calc-aqua_rat', 'test')),\n",
      " (('calc-math_qa', 'train'), ('calc-aqua_rat', 'validation')),\n",
      " (('calc-math_qa', 'train'), ('calc-math_qa', 'test')),\n",
      " (('calc-math_qa', 'train'), ('calc-math_qa', 'validation')),\n",
      " (('calc-math_qa', 'train'), ('calc-ape210k', 'test')),\n",
      " (('calc-math_qa', 'train'), ('calc-ape210k', 'validation')),\n",
      " (('calc-math_qa', 'train'), ('calc-mawps', 'validation')),\n",
      " (('calc-math_qa', 'train'), ('calc-mawps', 'test')),\n",
      " (('calc-math_qa', 'train'), ('calc-svamp', 'test')),\n",
      " (('calc-ape210k', 'train'), ('calc-gsm8k', 'test')),\n",
      " (('calc-ape210k', 'train'), ('calc-aqua_rat', 'test')),\n",
      " (('calc-ape210k', 'train'), ('calc-aqua_rat', 'validation')),\n",
      " (('calc-ape210k', 'train'), ('calc-math_qa', 'test')),\n",
      " (('calc-ape210k', 'train'), ('calc-math_qa', 'validation')),\n",
      " (('calc-ape210k', 'train'), ('calc-ape210k', 'test')),\n",
      " (('calc-ape210k', 'train'), ('calc-ape210k', 'validation')),\n",
      " (('calc-ape210k', 'train'), ('calc-mawps', 'validation')),\n",
      " (('calc-ape210k', 'train'), ('calc-mawps', 'test')),\n",
      " (('calc-ape210k', 'train'), ('calc-svamp', 'test')),\n",
      " (('calc-mawps', 'train'), ('calc-gsm8k', 'test')),\n",
      " (('calc-mawps', 'train'), ('calc-aqua_rat', 'test')),\n",
      " (('calc-mawps', 'train'), ('calc-aqua_rat', 'validation')),\n",
      " (('calc-mawps', 'train'), ('calc-math_qa', 'test')),\n",
      " (('calc-mawps', 'train'), ('calc-math_qa', 'validation')),\n",
      " (('calc-mawps', 'train'), ('calc-ape210k', 'test')),\n",
      " (('calc-mawps', 'train'), ('calc-ape210k', 'validation')),\n",
      " (('calc-mawps', 'train'), ('calc-mawps', 'validation')),\n",
      " (('calc-mawps', 'train'), ('calc-mawps', 'test')),\n",
      " (('calc-mawps', 'train'), ('calc-svamp', 'test'))]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "check_leaks = []\n",
    "for ds_name_1, ds_split_name_1 in dss.keys():\n",
    "    for ds_name_2, ds_split_name_2 in dss.keys():\n",
    "        if ds_split_name_1 == \"train\" and ds_split_name_2 != \"train\":\n",
    "            check_leaks.append(((ds_name_1, ds_split_name_1), (ds_name_2, ds_split_name_2)))\n",
    "\n",
    "\n",
    "pprint.pprint(check_leaks)\n",
    "print(len(check_leaks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = {}\n",
    "\n",
    "with joblib.Parallel(n_jobs=-1) as parallel:\n",
    "    jobs = (joblib.delayed(check_leak)(bows[ds_1], bows[ds_2]) for ds_1, ds_2 in check_leaks)\n",
    "    results = parallel(jobs)\n",
    "    for (ds_train, ds_eval), leak_candidates in zip(check_leaks, results):\n",
    "        candidates[ds_train, ds_eval] = leak_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.71% of calc-aqua_rat/test             examples appear similar to some examples in calc-aqua_rat/train\n",
      "-> 1.36% of calc-aqua_rat/train         examples would have to be dropped\n",
      "\n",
      "25.59% of calc-aqua_rat/validation       examples appear similar to some examples in calc-aqua_rat/train\n",
      "-> 0.33% of calc-aqua_rat/train         examples would have to be dropped\n",
      "\n",
      "97.49% of calc-math_qa/test              examples appear similar to some examples in calc-aqua_rat/train\n",
      "-> 24.56% of calc-aqua_rat/train         examples would have to be dropped\n",
      "\n",
      "97.00% of calc-math_qa/validation        examples appear similar to some examples in calc-aqua_rat/train\n",
      "-> 30.43% of calc-aqua_rat/train         examples would have to be dropped\n",
      "\n",
      "7.87% of calc-aqua_rat/test             examples appear similar to some examples in calc-math_qa/train\n",
      "-> 0.99% of calc-math_qa/train          examples would have to be dropped\n",
      "\n",
      "7.48% of calc-aqua_rat/validation       examples appear similar to some examples in calc-math_qa/train\n",
      "-> 0.25% of calc-math_qa/train          examples would have to be dropped\n",
      "\n",
      "85.17% of calc-math_qa/test              examples appear similar to some examples in calc-math_qa/train\n",
      "-> 42.85% of calc-math_qa/train          examples would have to be dropped\n",
      "\n",
      "84.98% of calc-math_qa/validation        examples appear similar to some examples in calc-math_qa/train\n",
      "-> 52.23% of calc-math_qa/train          examples would have to be dropped\n",
      "\n",
      "59.77% of calc-ape210k/test              examples appear similar to some examples in calc-ape210k/train\n",
      "-> 20.94% of calc-ape210k/train          examples would have to be dropped\n",
      "\n",
      "59.69% of calc-ape210k/validation        examples appear similar to some examples in calc-ape210k/train\n",
      "-> 20.91% of calc-ape210k/train          examples would have to be dropped\n",
      "\n",
      "77.88% of calc-mawps/validation          examples appear similar to some examples in calc-mawps/train\n",
      "-> 63.78% of calc-mawps/train            examples would have to be dropped\n",
      "\n",
      "77.50% of calc-mawps/test                examples appear similar to some examples in calc-mawps/train\n",
      "-> 56.16% of calc-mawps/train            examples would have to be dropped\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "print_examples = False\n",
    "\n",
    "for (ds_train, ds_eval), (train_sim, eval_sim) in candidates.items():\n",
    "    is_mostly_formula_problem = (dss[ds_eval][\"question_simplified\"].apply(len) / dss[ds_eval][\"question\"].apply(len)) < 0.5\n",
    "    # example of mostly_formula_problem is: Solve 2x + 3x^2 + 8/5 = 1295\n",
    "    # on those examples, we don't want to check for similarity on words\n",
    "    sus_mask = (eval_sim.values > threshold) # has shape (len_eval, top_k)\n",
    "    sus_mask[is_mostly_formula_problem] = False\n",
    "    suspicious_frac = sus_mask.any(dim=1).float().mean().item()\n",
    "    if suspicious_frac > 0.05:\n",
    "        print(f\"{suspicious_frac:.2%} of {'/'.join(ds_eval):<30} examples appear similar to some examples in {'/'.join(ds_train)}\")\n",
    "        sus_mask_in_train = (train_sim.values > threshold).any(dim=1).float().mean().item()\n",
    "        print(f\"-> {sus_mask_in_train:.2%} of {'/'.join(ds_train):<27} examples would have to be dropped\")\n",
    "        print()\n",
    "        if not print_examples:\n",
    "            continue\n",
    "        all_sus_eval_idxs, train_nth_similar = sus_mask.nonzero(as_tuple=True)\n",
    "        sample = torch.randint(0, len(all_sus_eval_idxs), (10,))\n",
    "        sampled_sus_eval_idxs = all_sus_eval_idxs[sample]\n",
    "        sampled_train_nth_similar = train_nth_similar[sample]\n",
    "        sampled_eval_questions = dss[ds_eval][\"question\"].iloc[sampled_sus_eval_idxs]\n",
    "        sampled_train_questions = dss[ds_train][\"question\"].iloc[eval_sim.indices[sampled_sus_eval_idxs, sampled_train_nth_similar]]\n",
    "        sampled_similarities = eval_sim.values[sampled_sus_eval_idxs, sampled_train_nth_similar]\n",
    "        for eval_question, train_question, similarity in zip(sampled_eval_questions, sampled_train_questions, sampled_similarities):\n",
    "            print(\"  eval: \", eval_question)\n",
    "            print(\"  train:\", train_question)\n",
    "            print(f\"  {similarity=:.2f}\")\n",
    "            print()\n",
    "\n",
    "        print()\n",
    "        print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data leaks:\n",
    "# aqua_rat train -> math_qa test + validation # math_qa is basically whole a subset of train aqua_rat\n",
    "# math_qa train -> math_qa test + validation\n",
    "# ape210k train -> ape210k test + validation\n",
    "# mawps train -> mawps test + validation\n",
    "\n",
    "# Fair evaluation for models trained on aquarat+ape210k+gsm8k+mathqa:\n",
    "# - don't eval on mathqa at all -> remove completely from latex table\n",
    "# - evaluation on gsm8k is ok\n",
    "# - need to evaluate on svamp and mawps -> we don't need to filter anything\n",
    "# - drop a lot of ape210k eval samples\n",
    "# - drop some aqua_rat eval samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gadgets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
